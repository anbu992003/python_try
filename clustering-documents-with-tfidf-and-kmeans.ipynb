{"cells":[{"metadata":{"_uuid":"0718f7143904159b620a449a4d19fe3ea6c7b11d"},"cell_type":"markdown","source":"# Overview\nIn this workbook, we'll leverage Sklearn's TFIDF vectorizer and MiniBatchKmeans to perform some simple document clustering. After which , we'll plot the clusters using PCA and TSNE, then show the top keywords in each cluster."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e68b26a96c179b6bd5b8e7f6e509cf2bae6c3bf2"},"cell_type":"markdown","source":"## Importing the data\nImporting the data is simple with Pandas. The source file is a newline delimited JSON file"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_json('../input/combined.json', lines=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"792e7c26f00e20f59e0908f96845305252921ff6"},"cell_type":"markdown","source":"## Extracting keywords\nHere I use the TfidfVectorizer since I'm hoping the IDF score will pull out unique words that I can use in clustering. There are lots of options to explore here to get different results, including CountVectorizer"},{"metadata":{"trusted":true,"_uuid":"a1e3d5c09accb60b684f8b22aa6a4f2544469886"},"cell_type":"code","source":"tfidf = TfidfVectorizer(\n    min_df = 5,\n    max_df = 0.95,\n    max_features = 8000,\n    stop_words = 'english'\n)\ntfidf.fit(data.contents)\ntext = tfidf.transform(data.contents)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10928181e141e10137ad9c7990f28bc2c8ed0cfa"},"cell_type":"markdown","source":"## Finding Optimal Clusters\nClustering is an unsupervised operation, and KMeans requires that we specify the number of clusters. One simple approach is to plot the SSE for a range of cluster sizes. We look for the \"elbow\" where the SSE begins to level off. MiniBatchKMeans introduces some noise so I raised the batch and init sizes higher. Unfortunately the regular Kmeans implementation is too slow. You'll notice different random states will generate different charts. Here I chose 14 clusters."},{"metadata":{"trusted":true,"_uuid":"67d51d6735d9386c256b9b03fc431555507a6c2a"},"cell_type":"code","source":"def find_optimal_clusters(data, max_k):\n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')\n    \nfind_optimal_clusters(text, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f03b074bcc9505a6700bcfe90724544f840ff8ee"},"cell_type":"code","source":"clusters = MiniBatchKMeans(n_clusters=14, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2326f23c16d679d71d214f3d2a966c568a046aa"},"cell_type":"markdown","source":"## Plotting Clusters\nHere we plot the clusters generated by our KMeans operation. One plot uses PCA which is better at capturing global structure of the data. The other uses TSNE which is better at capturing relations between neighbors. In order to speed up the process with TSNE, I sample from 3,000 documents and perform a PCA 50 dimension reduction on the data first. Next I show a scatterplot further sampling the sample down to 300 points."},{"metadata":{"trusted":true,"_uuid":"f4f81096ef9a89c5fb8b60908edbbaec415a6be2"},"cell_type":"code","source":"def plot_tsne_pca(data, labels):\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n    \n    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))\n    \n    \n    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    ax[0].set_title('PCA Cluster Plot')\n    \n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster Plot')\n    \nplot_tsne_pca(text, clusters)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5977476abc9118892d86f3b3aeb69de54c1623c7"},"cell_type":"markdown","source":"## Top Keywords\nLastly, we'll cycle through the clusters and print out the top keywords based on their TFIDF score to see if we can spot any trends. I'll do this by computing an average value across all dimensions in Pandas, grouped by the cluster label. Using numpy, finding the top words is simply sorting the average values for each row, and taking the top N.\n\nYou can see that we have a pretty good result. Topics including exploitation of children, tax fraud, civil rights, and environmental issues can be inferred from the top keywords. Other interesting approaches to this might include LDA topic modeling or possibly working with pre-trained word embeddings."},{"metadata":{"trusted":true,"_uuid":"3c9db70cff067ca3c7d7f79063086c566f21eb18"},"cell_type":"code","source":"def get_top_keywords(data, clusters, labels, n_terms):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    for i,r in df.iterrows():\n        print('\\nCluster {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n            \nget_top_keywords(text, clusters, tfidf.get_feature_names(), 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a2e21a659835bfbe41de33970a2d681769017d4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}