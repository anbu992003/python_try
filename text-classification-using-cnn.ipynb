{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33f5679b5e95039a32bd91c4d9fe6e55c3e17462"
   },
   "source": [
    "**INTRODUCTION**\n",
    "\n",
    "Hi all, this kernel is an intorduction to text classification using deep leanring. It took some time for the deep learning approaches to make a mark on textual data but since then the impact of deep learning on NLP has had a vertical graph. \n",
    "\n",
    "In this kernel we will get our hands dirty with a well in demand problem of text/document classification, around 2014 yoon kim et al. started to experiment with the relevance of CNN in the field of NLP and since then there has been no looking back. In the paper \"[Convolutional Neural Networks for Sentence Classification](http://arxiv.org/pdf/1408.5882.pdf)\" yoon kim et al. experiments with multiple CNN models (single channel, multiple channel) on top of word embeddings for text classification.\n",
    "\n",
    "For the sake of simplicity we will start off with a single channel model with pretrasined Glove embeddings. The data set used is the famous [20_newsgroup dataset](http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html)(original dataset link).\n",
    "\n",
    "In this kernel we will first learn about the processing of dataset, followed by a keras implementation of text classification using the preexisting Glove embeddings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a80121ade4ecf6c4caa264a5fd5cd5981778119a"
   },
   "source": [
    "**THE APPROACH**\n",
    "\n",
    "The idea presented follows a flow like : \n",
    "<a href=\"https://imgur.com/xLrP6IM\"><img src=\"https://i.imgur.com/xLrP6IM.png\" title=\"source: imgur.com\" style=\"width:400px;height:600px;\"/></a>\n",
    "\n",
    "\n",
    "We basically add different convolution layers of filter sizes [3, 4, 5], this somewhat emulates different skip-gram models where different filter sizes essentially means the number of words the filter is being applied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "0750388a2837b4425e5f1dd11da60a8f9b30c7b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "daf453aefbb50f26e525042203cfcbf4b3976c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\t\t  rec.autos\t      sci.space\r\n",
      "comp.graphics\t\t  rec.motorcycles     soc.religion.christian\r\n",
      "comp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns\r\n",
      "comp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast\r\n",
      "comp.sys.mac.hardware\t  sci.crypt\t      talk.politics.misc\r\n",
      "comp.windows.x\t\t  sci.electronics     talk.religion.misc\r\n",
      "misc.forsale\t\t  sci.med\r\n"
     ]
    }
   ],
   "source": [
    "# just to make sure the dataset is added properly \n",
    "!ls '../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "35297be0266ec89dc1786312025a26458be584d6"
   },
   "outputs": [],
   "source": [
    "# the dataset path\n",
    "TEXT_DATA_DIR = r'../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n",
    "#the path for Glove embeddings\n",
    "GLOVE_DIR = r'../input/glove6b/'\n",
    "# make the max word length to be constant\n",
    "MAX_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "# the percentage of train test split to be applied\n",
    "VALIDATION_SPLIT = 0.20\n",
    "# the dimension of vectors to be used\n",
    "EMBEDDING_DIM = 100\n",
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "embedding_dim = 100\n",
    "# dropout probability\n",
    "drop = 0.5\n",
    "batch_size = 30\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa68305c64bc844cd86e776b3cb37e3661f6f652"
   },
   "source": [
    "**DATASET STRUCTURE**\n",
    "\n",
    "The dataset is present in a hierarchical structure, i.e. all files of a given class are located in their respective folders and each datapoint has its own '.txt' file.\n",
    "\n",
    "* First we go through the entire dataset to build our text list and label list. \n",
    "* Followed by this we tokenize the entire data using Tokenizer, which is a part of keras.preprocessing.text.\n",
    "* We then add padding to the sequences to make them of a uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8f9972b8b95a38df4e08227b5a638bd675d7c945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}\n",
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "## preparing dataset\n",
    "\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "                t = f.read()\n",
    "                i = t.find('\\n\\n')  # skip header\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                texts.append(t)\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "print(labels_index)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f3058f0c6703374a384d8720712cb2151e44e8ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words : 174074\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences =  tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "fc1c709458e9f4eb338a40c40c85dedba29c6fe8"
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ba8039ec130a51f64bad77c718a7f2e91e13d19"
   },
   "source": [
    "Since we have our train-validation split ready, our next step is to create an embedding matrix from the precomputed Glove embeddings.\n",
    "For convenience we are freezing the embedding layer i.e we will not be fine tuning the word embeddings. Feel free to test it out for better accuracy on very specific examples. From what can be seen, the Glove embeddings are universal features and tend to perform great in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0620c11d2dab62329f250ecad40bcefbf57a7134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "07d064695cf65aaba497d6bb0dbd14dea220d533"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "74abe6ec0048d25c6169081f7cd409359588aee0"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e499119a397f180258ab0e2b8c5a6b47ef98fc7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1000, 100)\n",
      "(?, 1000, 100, 1)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    17407500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1000, 100, 1) 0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 998, 1, 512)  154112      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 997, 1, 512)  205312      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 996, 1, 512)  256512      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 512)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1536)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           30740       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 18,054,176\n",
      "Trainable params: 646,676\n",
      "Non-trainable params: 17,407,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "print(embedding.shape)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\n",
    "print(reshape.shape)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=20, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c3f99fb84e45c5fdf63607020c346902c340a31a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 29s 2ms/step - loss: 3.4850 - acc: 0.1086 - val_loss: 2.1056 - val_acc: 0.4819\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48187, saving model to weights_cnn_sentece.hdf5\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 24s 1ms/step - loss: 2.3174 - acc: 0.2982 - val_loss: 1.6170 - val_acc: 0.5999\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.48187 to 0.59990, saving model to weights_cnn_sentece.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc1e7169b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Traning Model...\")\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6dd11fdc4f1d2898adb4103d7794a7d4a3919d4c"
   },
   "source": [
    "I hope this Kernel was helpful for you, any sort of feedback and comments are appreciated. Feel free to reach out in case something is unclear.\n",
    "the entire code is also uploaded on my github : https://github.com/au1206/Convolutional-Neural-Networks-for-Sentence-Classification\n",
    "\n",
    "\n",
    "Until next time, Happy learning :) . . .. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4da2bc289e1d27d5225f68cb33352347e737c8a4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
